{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fH6IfYRvrQ5J"
   },
   "outputs": [],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvRVLB9VrYkj"
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import GloVe\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device} is used\")\n",
    "\n",
    "# seed everything\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMmEeiSBehK3"
   },
   "source": [
    "Run one of the 2 cells below. The first one can only be run with internet access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfjj7e_Sejil"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "glove = GloVe(name=\"840B\", dim=embedding_dim)\n",
    "\n",
    "def lineToTensor(line):\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", line)\n",
    "    tensor = torch.tensor([glove.stoi[w] for w in words if w in glove.stoi], dtype=torch.long)\n",
    "    return glove.vectors[tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xe0hgig5rlfS"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "GLOVE_EMB = 'glove.840B.300d.txt'\n",
    "embeddings_index = {}\n",
    "f = codecs.open(GLOVE_EMB, encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(\" \")\n",
    "    word = value = values[0]\n",
    "    coefs = np.asarray(values[1:51], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "# Turn a line of words into the curresponding indices\n",
    "def lineToTensor(line):\n",
    "    words = re.findall(r\"[\\w']+|[.,!?;]\", line)\n",
    "    tensor = torch.tensor([embeddings_index[w] for w in words if w in embeddings_index])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5wKJzmmoequy"
   },
   "outputs": [],
   "source": [
    "def pad_tensor(tensor, new_size):\n",
    "    num_pad = new_size - tensor.shape[0]\n",
    "    pad = (0, 0, 0, num_pad) # pad last dim by (0, 0) and 2nd to last by (0, num_pad)\n",
    "    return nn.functional.pad(tensor, pad, \"constant\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AN1WYjfxTOV6"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')  # 2834 rows, 6 cols\n",
    "test_data  = pd.read_csv('test.csv')   #    7 rows, 4 cols\n",
    "\n",
    "train_data['encodings'] = train_data.apply (lambda row: lineToTensor(row['excerpt']), axis=1)\n",
    "train_data['encoding_len'] = train_data.apply (lambda row: row['encodings'].shape[0], axis=1)\n",
    "test_data['encodings'] = test_data.apply (lambda row: lineToTensor(row['excerpt']), axis=1)\n",
    "test_data['encoding_len'] = test_data.apply (lambda row: row['encodings'].shape[0], axis=1)\n",
    "\n",
    "# Pad tensor\n",
    "max_tensor_len = max([tensor.shape[0] for tensor in train_data['encodings']])\n",
    "train_data['encodings'] = train_data.apply (lambda row: pad_tensor(row['encodings'], max_tensor_len), axis=1)\n",
    "test_data['encodings'] = test_data.apply (lambda row: pad_tensor(row['encodings'], max_tensor_len), axis=1)\n",
    "\n",
    "# Split training and validation\n",
    "train_X, val_X, train_Y, val_Y = train_test_split(\n",
    "    train_data[['encodings', 'encoding_len']],\n",
    "    train_data['target'],\n",
    "    test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cl_V3rl5rvbP"
   },
   "outputs": [],
   "source": [
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, X, labels=None):\n",
    "        self.encodings = X['encodings'].to_numpy()\n",
    "        self.word_lengths = X['encoding_len'].to_numpy()\n",
    "        if labels is not None:\n",
    "            self.labels = labels.to_numpy()\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.labels is not None:\n",
    "            return (self.encodings[idx], self.word_lengths[idx]), torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return (self.encodings[idx], self.word_lengths[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzoFhCKT1JXy"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim, device):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.att_weights = nn.Parameter(torch.Tensor(hidden_dim, 1), requires_grad=True)\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_dim)\n",
    "        for weight in self.att_weights:\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def get_mask(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        batch_size, max_len = inputs.size()[:2]\n",
    "            \n",
    "        # Apply attention layer\n",
    "        weights = torch.bmm(\n",
    "            inputs,                                                # (batch_size, max_len, hidden_dim)\n",
    "            self.att_weights.unsqueeze(0).repeat(batch_size, 1, 1) # (batch_size, hidden_dim, 1)\n",
    "        ) # (batch_size, max_len, 1)\n",
    "        attentions = torch.softmax(nn.functional.relu(weights.squeeze()), dim=-1) # (batch_size, max_len)\n",
    "\n",
    "        # Create mask based on the input lengths\n",
    "        mask = torch.ones(attentions.size(), requires_grad=False).to(self.device)\n",
    "        for i, l in enumerate(lengths):\n",
    "            if l < max_len:\n",
    "                mask[i, l:] = 0\n",
    "\n",
    "        # Apply mask and renormalize attention scores\n",
    "        masked = attentions * mask\n",
    "        sum_per_row = masked.sum(-1).unsqueeze(-1)\n",
    "        attentions = masked.div(sum_per_row)\n",
    "\n",
    "        # Get the final representations of the input\n",
    "        representations = torch.mul(inputs, attentions.unsqueeze(-1).expand_as(inputs)).sum(1).squeeze()\n",
    "\n",
    "        return representations, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSPA0xOKrxE5"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, dropout, device):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.attention = Attention(hidden_dim, device)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, input_sequence, sequence_length):\n",
    "        input_sequence = self.dropout(input_sequence)\n",
    "        lstm_out, (ht, ct) = self.lstm(input_sequence)\n",
    "        att_out, _ = self.attention(lstm_out, sequence_length)\n",
    "        return self.linear(att_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_PQOgvec_e4"
   },
   "outputs": [],
   "source": [
    "# Train for 1 epoch\n",
    "def train(model, optimizer, loss_function, train_loader, device):\n",
    "    model.train() # put to train mode\n",
    "    total_train_loss = 0\n",
    "    for (seq, seq_len), labels in train_loader:\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq, seq_len)\n",
    "        loss = loss_function(y_pred, labels.unsqueeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    return total_train_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5rIUWJdj-zR"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, device):\n",
    "    total_val_loss = 0\n",
    "    model.eval() # put to eval mode\n",
    "    for (seq, seq_len), labels in val_loader:\n",
    "        with torch.no_grad():\n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            y_pred = model(seq, seq_len)\n",
    "            loss = loss_function(y_pred, labels.unsqueeze(-1))\n",
    "            total_val_loss += loss.item()\n",
    "    return total_val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7U6aWq3yGsLt"
   },
   "outputs": [],
   "source": [
    "def predict(model, test_loader, device):\n",
    "    y_preds = []\n",
    "    for seq, seq_len in test_loader:\n",
    "        with torch.no_grad():\n",
    "            seq = seq.to(device)\n",
    "            preds = model(seq, seq_len)\n",
    "            y_preds.append(preds.detach().cpu().numpy())\n",
    "    return np.concatenate(y_preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XatcGksUr0az"
   },
   "outputs": [],
   "source": [
    "hidden_dim    = 256\n",
    "learning_rate = 0.00003\n",
    "weight_decay  = 1e-8\n",
    "dropout       = 0.2\n",
    "n_epoch       = 100\n",
    "batch_size    = 16\n",
    "\n",
    "model_loss = np.zeros((n_epoch, 2))\n",
    "\n",
    "commonlit_train = CommonLitDataset(train_X, train_Y)\n",
    "commonlit_val = CommonLitDataset(val_X, val_Y)\n",
    "train_loader = DataLoader(commonlit_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(commonlit_val, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Build model\n",
    "model = LSTM(embedding_dim, hidden_dim, dropout, device).to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Train model\n",
    "for i in range(n_epoch):\n",
    "    print(\".\", end=\"\", flush=True)\n",
    "    avg_train_loss = train(model, optimizer, loss_function, train_loader, device)\n",
    "    avg_val_loss = evaluate(model, val_loader, device)\n",
    "    model_loss[i][0] = avg_train_loss\n",
    "    model_loss[i][1] = avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqzfKdu8BQ45"
   },
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "plt.figure()\n",
    "plt.plot(model_loss[:,0], label='Training error')\n",
    "plt.plot(model_loss[:,1], label='Val Error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PXkdj3NHUyp"
   },
   "outputs": [],
   "source": [
    "# Train with full dataset\n",
    "n_epoch = 50\n",
    "commonlit_train = CommonLitDataset(train_data[['encodings', 'encoding_len']], train_data['target'])\n",
    "train_loader = DataLoader(commonlit_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = LSTM(embedding_dim, hidden_dim, dropout, device).to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    avg_train_loss = train(model, optimizer, loss_function, train_loader, device)\n",
    "    if i % 10 == 0:\n",
    "        print('Iteration %d | Loss = %.4f' % (i, avg_train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsSz-NfdGxEX"
   },
   "outputs": [],
   "source": [
    "# Predict blind test set\n",
    "commonlit_test = CommonLitDataset(test_data[['encodings', 'encoding_len']])\n",
    "test_loader = DataLoader(commonlit_test, batch_size=batch_size, shuffle=False)\n",
    "test_data['target'] = predict(model, test_loader, device)\n",
    "submission = test_data[['id', 'target']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "glove_lstm_att.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
